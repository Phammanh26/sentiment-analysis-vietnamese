{
	"name": "statistic",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "manhpv8",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "49d9b04a-4321-43e2-8152-286f552d8c26"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/813e37e4-73d3-4513-a700-f9581423d59c/resourceGroups/utop/providers/Microsoft.Synapse/workspaces/synapse-utop/bigDataPools/manhpv8",
				"name": "manhpv8",
				"type": "Spark",
				"endpoint": "https://synapse-utop.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/manhpv8",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.1",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from xmlrpc.client import Boolean\n",
					"import numpy as np\n",
					"import json\n",
					"import warnings\n",
					"import pandas as pd\n",
					"\n",
					"import argparse\n",
					"\n",
					"import time\n",
					"warnings.filterwarnings(\"ignore\")\n",
					"\n",
					"def extract_num_record(df: pd.DataFrame, file_name: str, column_name: str):\n",
					"    \n",
					"    output = []\n",
					"\n",
					"    df = df[df[column_name].isna() == False]\n",
					"\n",
					"    # Extract year and month\n",
					"    df['year'] = pd.DatetimeIndex(df[column_name]).year\n",
					"    df['year'] = df['year'].apply(lambda x: str(int(x)))\n",
					"    df['month'] = pd.DatetimeIndex(df[column_name]).month\n",
					"    df['month'] = df['month'].apply(lambda x: str(int(x)))\n",
					"\n",
					"    # Merge year_month values\n",
					"    dict_year_month = df[[\"year\", \"month\"]].apply(\"-\".join, axis=1).value_counts().to_dict()\n",
					"    uni_years = df['year'].unique()\n",
					"   \n",
					"    for year in uni_years:\n",
					"        temp = [file_name, year]\n",
					"        for month in range(1,13):\n",
					"            year_month = \"-\".join((str(year), str(month)))\n",
					"            year_month_count = 0\n",
					"\n",
					"            if year_month in dict_year_month:\n",
					"                year_month_count = dict_year_month[year_month]\n",
					"\n",
					"            temp.append(year_month_count)\n",
					"        output.append(temp)\n",
					"    \n",
					"    return  output\n",
					"\n",
					" ## employee datasets\n",
					"def extract_employee(emp_df: pd.DataFrame, file_name: str):\n",
					"\n",
					"    output = []\n",
					"    current_year = time.strftime(\"%Y\")\n",
					"    current_month = time.strftime(\"%m\")\n",
					"    output = [file_name, current_year]\n",
					"\n",
					"    for month in range(1,13):\n",
					"        if month == int(current_month):\n",
					"            output.append(len(emp_df))\n",
					"        else:\n",
					"            output.append(0)\n",
					"    return output\n",
					"\n",
					"## tms datasetsd    \n",
					"def extract_tms(df: pd.DataFrame, file_name: str, columns = None):\n",
					"\n",
					"    output = []\n",
					"\n",
					"    year = file_name.split(\"_\")[1]\n",
					"    output = [file_name, year]\n",
					"\n",
					"    df = df[columns]\n",
					"    list_count_notzero = df[df > 0].count().tolist()\n",
					"    \n",
					"    output.extend(list_count_notzero)\n",
					"    return output\n",
					"\n",
					"def read_csv(folder = None, file_name = None, p_filename = None):\n",
					"    if p_filename == None:\n",
					"        path= folder + file_name + '.csv'\n",
					"        df = pd.read_csv(path)\n",
					"    else:\n",
					"         df = pd.read_csv(p_filename)\n",
					"    return df\n",
					"\n",
					"\n",
					"\n",
					"if __name__ == '__main__':\n",
					"    \n",
					"    # setup column name dataframe\n",
					"    months =  [month for month in range(1, 13)]\n",
					"    col_infor = ['data', 'year']\n",
					"    columns =col_infor +  months\n",
					"\n",
					"    #Initialize parse\n",
					"    parser = argparse.ArgumentParser(description=\"Optional app description\")\n",
					"    # Add Arguments\n",
					"    parser.add_argument(\"--folder\", type=str, default=\"data/\")\n",
					"    parser.add_argument(\"--path_config\", type=str, default=\"config/extract_columns.json\")\n",
					"    parser.add_argument(\"--save_report_to\", type=str, default=\"report/output.csv\")\n",
					"    \n",
					"    parser.add_argument(\"--path_file\", type=str, default=None)\n",
					"    parser.add_argument(\"--colname\", type=str, default=None)\n",
					"    parser.add_argument(\"--all\", type=bool, default= False)\n",
					"    \n",
					"    # Parse\n",
					"    args = parser.parse_args()\n",
					"    #declare\n",
					"    folder = args.folder\n",
					"    path_config = args.path_config\n",
					"    save_report_to = args.save_report_to\n",
					"    path_file = args.path_file\n",
					"    colname = args.colname\n",
					"    # get filename and key date be extracted\n",
					"    with open(path_config, encoding ='utf-8') as f:\n",
					"        columns_config= json.load(f)\n",
					"    \n",
					"\n",
					"    if path_file != None:\n",
					"        outputs = []\n",
					"        df = read_csv(p_filename = path_file)\n",
					"        filename = path_file\n",
					"        if colname == \"list\":\n",
					"            colname = columns_config[\"list_columns\"]\n",
					"            # tms datasets  \n",
					"            if len(colname) > 0:\n",
					"                output = extract_tms(df, filename, colname)\n",
					"            # employee datasets   \n",
					"            elif len(colname) == 0:\n",
					"                output = extract_employee(df, filename)\n",
					"            outputs.append(output)\n",
					"        \n",
					"        # common datasets\n",
					"        elif colname != \"list\":\n",
					"            output = extract_num_record(df, filename, colname)\n",
					"            outputs.extend(output)\n",
					"        df = pd.DataFrame(outputs, columns = columns)\n",
					"        print(df)\n",
					"\n",
					"   \n",
					"    if args.all == True:\n",
					"       \n",
					"\n",
					"        print(\"START EXTRACT!!!\")\n",
					"        outputs = []\n",
					"        for col_data in columns_config:\n",
					"            \n",
					"            filename = col_data\n",
					"            colname = columns_config[col_data]\n",
					"            df = read_csv(folder, filename)\n",
					"            \n",
					"            output = []\n",
					"            if type(colname) == list:\n",
					"                # tms datasets  \n",
					"                if len(colname) > 0:\n",
					"                    output = extract_tms(df, filename, colname)\n",
					"                # employee datasets   \n",
					"                elif len(colname) == 0:\n",
					"                    output = extract_employee(df, filename)\n",
					"                outputs.append(output)\n",
					"            \n",
					"            # common datasets\n",
					"            elif type(colname) == str:\n",
					"                output = extract_num_record(df, filename, colname)\n",
					"                outputs.extend(output)\n",
					"\n",
					"\n",
					"            df = pd.DataFrame(outputs, columns = columns)\n",
					"        print(\"FINISH EXTRACT!!!\")\n",
					"        ## save file statistic\n",
					"        df.to_csv(save_report_to, index= False)\n",
					"        print(\"SAVE DONE!!!\")"
				],
				"execution_count": 1
			}
		]
	}
}